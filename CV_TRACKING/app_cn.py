"""
é›¶æ ·æœ¬ç›®æ ‡è·Ÿè¸ª Gradio ç•Œé¢
"""
import gradio as gr
import cv2
import numpy as np
import time
import os
from pathlib import Path
from trackers.hybrid_tracker import HybridTracker


# å…¨å±€è·Ÿè¸ªå™¨å®ä¾‹
tracker = None
OUTPUT_DIR = Path("output")
OUTPUT_DIR.mkdir(exist_ok=True)

# ROI é€‰æ‹©çš„å…¨å±€çŠ¶æ€
roi_state = {
    'frame': None,
    'points': [],
    'bbox': None
}


def process_video(video_path, x1, y1, x2, y2):
    """
    ä¸»è·Ÿè¸ªæµç¨‹
    å‚æ•°:
        video_path: è¾“å…¥è§†é¢‘çš„è·¯å¾„
        x1, y1, x2, y2: è¾¹ç•Œæ¡†åæ ‡
    è¿”å›:
        output_video_path: è·Ÿè¸ªåçš„è§†é¢‘è·¯å¾„
        stats: è·Ÿè¸ªç»Ÿè®¡ä¿¡æ¯
    """
    global tracker
    
    # éªŒè¯è¾“å…¥
    if video_path is None:
        return None, "âŒ é”™è¯¯ï¼šè¯·å…ˆä¸Šä¼ è§†é¢‘ï¼"
    
    if x1 is None or y1 is None or x2 is None or y2 is None:
        return None, "âŒ é”™è¯¯ï¼šè¯·å…ˆé€‰æ‹©ç›®æ ‡åŒºåŸŸï¼"
    
    try:
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
    except:
        return None, "âŒ é”™è¯¯ï¼šè¾¹ç•Œæ¡†åæ ‡æ ¼å¼æ— æ•ˆï¼"
    
    # ç¡®ä¿æœ‰æ•ˆçš„è¾¹ç•Œæ¡†
    if x2 <= x1 or y2 <= y1:
        return None, "âŒ é”™è¯¯ï¼šæ— æ•ˆçš„è¾¹ç•Œæ¡†ï¼x2 å¿…é¡»å¤§äº x1ï¼Œy2 å¿…é¡»å¤§äº y1"
    
    init_bbox = (x1, y1, x2-x1, y2-y1)
    
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return None, "âŒ é”™è¯¯ï¼šæ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶ï¼"
    
    # è·å–è§†é¢‘å±æ€§
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # è¾“å‡ºè§†é¢‘å†™å…¥å™¨
    timestamp = int(time.time())
    output_path = OUTPUT_DIR / f"tracked_{timestamp}.mp4"
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
    
    # ä½¿ç”¨ç¬¬ä¸€å¸§åˆå§‹åŒ–è·Ÿè¸ªå™¨
    ret, first_frame = cap.read()
    if not ret:
        return None, "âŒ é”™è¯¯ï¼šæ— æ³•è¯»å–ç¬¬ä¸€å¸§ï¼"
    
    tracker = HybridTracker(device='cuda')
    tracker.init(first_frame, init_bbox)
    
    # åœ¨ç¬¬ä¸€å¸§ä¸Šç»˜åˆ¶åˆå§‹è¾¹ç•Œæ¡†
    vis_frame = first_frame.copy()
    cv2.rectangle(vis_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
    # "Initial Target" -> "åˆå§‹ç›®æ ‡"
    # æ³¨æ„ï¼šcv2.putText ä¸æ”¯æŒç›´æ¥ç»˜åˆ¶ä¸­æ–‡å­—ç¬¦ï¼Œè‹¥éœ€ä¸­æ–‡éœ€ä½¿ç”¨ PIL è½¬æ¢ã€‚
    # è¿™é‡Œä¸ºäº†ä¿æŒä»£ç ä¾èµ–ç®€å•ï¼Œæš‚æ—¶ä¿ç•™è‹±æ–‡æˆ–ä½¿ç”¨æ‹¼éŸ³ï¼Œæˆ–è€…å‡è®¾ç³»ç»Ÿæ”¯æŒã€‚
    # ä¸ºä¿è¯ç¨³å®šæ€§ï¼Œè¿™é‡Œç”¨è‹±æ–‡ "Initial Target" (å› ä¸ºOpenCVé»˜è®¤ä¸æ”¯æŒä¸­æ–‡)
    # å¦‚æœä¸€å®šè¦ä¸­æ–‡ï¼Œé€šå¸¸éœ€è¦ freetype æˆ– PILã€‚è¿™é‡Œæˆ‘ä¿ç•™è‹±æ–‡ä»¥é˜²ä¹±ç ï¼Œ
    # ä½†åœ¨æ³¨é‡Šä¸­è¯´æ˜ã€‚
    cv2.putText(vis_frame, "Initial Target", (x1, y1-10), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    out.write(vis_frame)
    
    # è·Ÿè¸ªç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_frames': total_frames,
        'tracked': 0,
        'lost': 0,
        'redetected': 0,
        'avg_fps': 0,
        'avg_confidence': 0
    }
    
    frame_idx = 1
    confidences = []
    start_time = time.time()
    
    print(f"ğŸ¯ å¼€å§‹è·Ÿè¸ªï¼Œåˆå§‹è¾¹ç•Œæ¡†: {init_bbox}")
    
    # å¤„ç†å‰©ä½™å¸§
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # è·Ÿè¸ªå¯¹è±¡
        bbox, confidence, status = tracker.update(frame)
        confidences.append(confidence)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if status == "REDETECTED":
            stats['redetected'] += 1
        elif status == "LOST":
            stats['lost'] += 1
        else:
            stats['tracked'] += 1
        
        # å¯è§†åŒ–ï¼ˆä»…å½“ç½®ä¿¡åº¦ > 0.15 æ—¶æ˜¾ç¤ºè¾¹ç•Œæ¡†ï¼‰
        vis_frame = frame.copy()
        if bbox is not None and confidence > 0.15:
            x, y, w, h = [int(v) for v in bbox]
            
            # åŸºäºç½®ä¿¡åº¦è®¾å®šé¢œè‰²
            if confidence > 0.7:
                color = (0, 255, 0)  # ç»¿è‰²
            elif confidence > 0.4:
                color = (0, 165, 255)  # æ©™è‰²
            else:
                color = (0, 0, 255)  # çº¢è‰²
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(vis_frame, (x, y), (x+w, y+h), color, 2)
            
            # çŠ¶æ€æ–‡æœ¬
            text = f"{status} | Conf: {confidence:.2f}"
            cv2.putText(vis_frame, text, (x, y-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # å¸§è®¡æ•°å™¨ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼‰
        cv2.putText(vis_frame, f"Frame: {frame_idx}/{total_frames}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        out.write(vis_frame)
        frame_idx += 1
        
        # è¿›åº¦æŒ‡ç¤ºå™¨
        if frame_idx % 30 == 0:
            progress = (frame_idx / total_frames) * 100
            print(f"å¤„ç†è¿›åº¦: {progress:.1f}% | çŠ¶æ€: {status} | ç½®ä¿¡åº¦: {confidence:.2f}")
    
    # æ¸…ç†èµ„æº
    cap.release()
    out.release()
    
    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    elapsed_time = time.time() - start_time
    stats['avg_fps'] = total_frames / elapsed_time if elapsed_time > 0 else 0
    stats['avg_confidence'] = np.mean(confidences) if confidences else 0
    
    stats_text = f"""
    âœ… è·Ÿè¸ªå®Œæˆï¼
    ğŸ“Š ç»Ÿè®¡æ•°æ®:
    - æ€»å¸§æ•°: {stats['total_frames']}
    - æˆåŠŸè·Ÿè¸ªå¸§æ•°: {stats['tracked']}
    - é‡æ£€æµ‹æ¬¡æ•°: {stats['redetected']}
    - ä¸¢å¤±å¸§æ•°: {stats['lost']}
    - å¹³å‡ FPS: {stats['avg_fps']:.1f}
    - å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.3f}    
    ğŸ¥ è¾“å‡ºæ–‡ä»¶: {output_path.name}
    """
    
    return str(output_path), stats_text


def load_first_frame(video_path):
    global roi_state
    if video_path is None:
        roi_state['frame'] = None
        roi_state['points'] = []
        roi_state['bbox'] = None
        return None, None, None, None, None, None, None
    cap = cv2.VideoCapture(video_path)
    ret, frame = cap.read()
    cap.release()
    
    if ret:
        # å…¨å±€å­˜å‚¨å¸§
        roi_state['frame'] = frame
        roi_state['points'] = []
        roi_state['bbox'] = None
        
        # å°† BGR è½¬æ¢ä¸º RGB ä»¥ä¾› Gradio ä½¿ç”¨
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        height, width = frame.shape[:2]
        
        # å°†è¾¹ç•Œæ¡†åˆå§‹åŒ–ä¸ºå¸§ä¸­å¿ƒçš„å››åˆ†ä¹‹ä¸€
        default_x1 = width // 4
        default_y1 = height // 4
        default_x2 = 3 * width // 4
        default_y2 = 3 * height // 4
        # åœ¨å¸§ä¸Šç»˜åˆ¶åˆå§‹è¾¹ç•Œæ¡†
        preview = rgb_frame.copy()
        cv2.rectangle(preview, (default_x1, default_y1), (default_x2, default_y2), (0, 255, 0), 2)
        # æ³¨æ„ï¼šæ­¤å¤„ä¸ºåœ¨Gradioä¸­æ˜¾ç¤ºçš„å›¾ç‰‡ï¼Œè‹¥OpenCVä¸æ”¯æŒä¸­æ–‡ï¼Œä¾ç„¶ä½¿ç”¨è‹±æ–‡æç¤º
        cv2.putText(preview, "Adjust bbox below or click two points", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        return preview, default_x1, default_y1, default_x2, default_y2, width, height
    
    return None, None, None, None, None, None, None


def update_bbox_preview(video_path, x1, y1, x2, y2):
    global roi_state
    if roi_state['frame'] is None:
        return None
    # è½¬æ¢ä¸º RGB
    preview = cv2.cvtColor(roi_state['frame'], cv2.COLOR_BGR2RGB).copy()
    # å¦‚æœè¾¹ç•Œæ¡†æœ‰æ•ˆåˆ™ç»˜åˆ¶
    try:
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
        if x2 > x1 and y2 > y1:
            cv2.rectangle(preview, (x1, y1), (x2, y2), (0, 255, 0), 3)
            # ç»˜åˆ¶åæ ‡æ–‡æœ¬
            cv2.putText(preview, f"ROI: ({x1},{y1}) to ({x2},{y2})", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            w, h = x2 - x1, y2 - y1
            cv2.putText(preview, f"Size: {w}x{h}", (10, 60),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    except:
        pass
    return preview


def handle_image_click(video_path, x1, y1, x2, y2, evt: gr.SelectData):
    """
    å¤„ç†å›¾åƒç‚¹å‡»äº‹ä»¶ä»¥è®¾ç½®è¾¹ç•Œæ¡†è§’ç‚¹
    """
    global roi_state
    
    if roi_state['frame'] is None:
        return x1, y1, x2, y2
    
    click_x, click_y = evt.index[0], evt.index[1]
    roi_state['points'].append((click_x, click_y))
    
    print(f"ğŸ–±ï¸ ç‚¹å‡»ç¬¬ {len(roi_state['points'])} æ¬¡: ({click_x}, {click_y})")
    
    # ç‚¹å‡»ä¸¤æ¬¡åï¼Œè®¾ç½®è¾¹ç•Œæ¡†
    if len(roi_state['points']) >= 2:
        p1, p2 = roi_state['points'][-2], roi_state['points'][-1]
        new_x1 = min(p1[0], p2[0])
        new_y1 = min(p1[1], p2[1])
        new_x2 = max(p1[0], p2[0])
        new_y2 = max(p1[1], p2[1])
        
        print(f"âœ“ è¾¹ç•Œæ¡†å·²è®¾å®š: ({new_x1},{new_y1}) è‡³ ({new_x2},{new_y2})")
        
        return new_x1, new_y1, new_x2, new_y2
    
    return x1, y1, x2, y2


# Gradio ç•Œé¢
with gr.Blocks(title="ğŸ¯ é›¶æ ·æœ¬ç›®æ ‡è·Ÿè¸ªå™¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ¯ é›¶æ ·æœ¬ç›®æ ‡è·Ÿè¸ªç³»ç»Ÿ (Zero-shot Object Tracking)
    
    **åŠŸèƒ½ç‰¹ç‚¹:**
    - âœ¨ è·Ÿè¸ªä»»æ„ç›®æ ‡ (é›¶æ ·æœ¬/Zero-shot)
    - ğŸš€ å®æ—¶æ€§èƒ½ (30-50 FPS)
    - ğŸ”„ é®æŒ¡æ¢å¤èƒ½åŠ› (æ”¯æŒ 3ç§’+ é®æŒ¡)
    - ğŸ¨ GPU åŠ é€Ÿ (CUDA)
    
    **ä½¿ç”¨è¯´æ˜:**
    1. ä¸Šä¼ ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ã€‚
    2. **æ–¹æ³• 1**: åœ¨è§†é¢‘å¸§ä¸Šç‚¹å‡» **ä¸¤ç‚¹** æ¥å®šä¹‰è¾¹ç•Œæ¡†ï¼ˆå·¦ä¸Šè§’å’Œå³ä¸‹è§’ï¼‰ã€‚
    3. **æ–¹æ³• 2**: è°ƒæ•´å›¾åƒä¸‹æ–¹çš„åæ ‡æ»‘å—ã€‚
    4. ç‚¹å‡» "å¼€å§‹è·Ÿè¸ª" (Start Tracking)ã€‚
    5. ä¸‹è½½è·Ÿè¸ªå®Œæˆçš„è§†é¢‘ã€‚
    """)
    
    with gr.Row():
        with gr.Column():
            video_input = gr.Video(label="ğŸ“¤ ä¸Šä¼ è§†é¢‘")
            
            frame_display = gr.Image(
                label="ğŸ¯ ç‚¹å‡»ä¸¤ç‚¹é€‰æ‹© ROIï¼ˆæˆ–ä½¿ç”¨ä¸‹æ–¹æ»‘å—è°ƒæ•´ï¼‰",
                interactive=False,
                type="numpy"
            )
            
            with gr.Row():
                x1_slider = gr.Number(label="X1 (å·¦)", value=0, precision=0)
                y1_slider = gr.Number(label="Y1 (é¡¶)", value=0, precision=0)
            
            with gr.Row():
                x2_slider = gr.Number(label="X2 (å³)", value=100, precision=0)
                y2_slider = gr.Number(label="Y2 (åº•)", value=100, precision=0)
            
            # éšè—çš„å°ºå¯¸çŠ¶æ€
            frame_width = gr.State(value=None)
            frame_height = gr.State(value=None)
            
            track_btn = gr.Button("ğŸš€ å¼€å§‹è·Ÿè¸ª", variant="primary", size="lg")
        
        with gr.Column():
            output_video = gr.Video(label="ğŸ“¥ è·Ÿè¸ªç»“æœè§†é¢‘")
            stats_output = gr.Textbox(label="ğŸ“Š è·Ÿè¸ªç»Ÿè®¡ä¿¡æ¯", lines=12)
    
    # äº‹ä»¶å¤„ç†
    video_input.change(
        fn=load_first_frame,
        inputs=[video_input],
        outputs=[frame_display, x1_slider, y1_slider, x2_slider, y2_slider, frame_width, frame_height]
    )
    
    # å½“æ»‘å—å˜åŒ–æ—¶æ›´æ–°é¢„è§ˆ
    for slider in [x1_slider, y1_slider, x2_slider, y2_slider]:
        slider.change(
            fn=update_bbox_preview,
            inputs=[video_input, x1_slider, y1_slider, x2_slider, y2_slider],
            outputs=[frame_display]
        )
    
    # å¤„ç†å›¾åƒç‚¹å‡»
    frame_display.select(
        fn=handle_image_click,
        inputs=[video_input, x1_slider, y1_slider, x2_slider, y2_slider],
        outputs=[x1_slider, y1_slider, x2_slider, y2_slider]
    )
    
    # å¼€å§‹è·Ÿè¸ª
    track_btn.click(
        fn=process_video,
        inputs=[video_input, x1_slider, y1_slider, x2_slider, y2_slider],
        outputs=[output_video, stats_output]
    )
    
    gr.Markdown("""
    ---
    ### ğŸ”§ æŠ€æœ¯æ ˆ:
    - **ä¸»è·Ÿè¸ªå™¨**: OpenCV CSRT (é€šé“ä¸ç©ºé—´å¯é æ€§è·Ÿè¸ªå™¨)
    - **è¿åŠ¨é¢„æµ‹**: å¡å°”æ›¼æ»¤æ³¢ (Kalman Filter, 8çŠ¶æ€æ’é€Ÿæ¨¡å‹)
    - **é‡æ£€æµ‹**: YOLO11n (CUDA åŠ é€Ÿï¼Œæœ¬åœ°æ¨¡å‹)
    - **é‡è¯†åˆ« (Re-ID)**: å¤šç‰¹å¾åŒ¹é… (é¢œè‰²ç›´æ–¹å›¾ + æ¨¡æ¿åŒ¹é… + ç©ºé—´é‚»è¿‘åº¦)
    
    ### ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–:
    - ç¬¬ 1 å±‚: CSRT ç”¨äºé€å¸§è·Ÿè¸ª (é€Ÿåº¦æœ€å¿«)
    - ç¬¬ 2 å±‚: ä½ç½®ä¿¡åº¦æœŸé—´ä½¿ç”¨å¡å°”æ›¼é¢„æµ‹
    - ç¬¬ 3 å±‚: è§¦å‘å¼ YOLO11 é‡æ£€æµ‹ (ä»…åœ¨éœ€è¦æ—¶è¿è¡Œ)
    - è‡ªé€‚åº”ç½®ä¿¡åº¦é˜ˆå€¼å®ç°æ™ºèƒ½åˆ‡æ¢
    """)


if __name__ == "__main__":
    demo.launch(share=False, server_name="0.0.0.0", server_port=3021)