"""
é›¶æ ·æœ¬ç›®æ ‡è·Ÿè¸ª Gradio ç•Œé¢ (TensorRT åŠ é€Ÿç‰ˆ - FP32)
"""
import gradio as gr
import cv2
import numpy as np
import time
import os
import sys
from pathlib import Path

# Add project root to path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

try:
    import tensorrt
    from trackers.ostrack_trt_tracker import OSTrackFerrariTRT
    print("ğŸš€ TensorRT detected! Using OSTrackFerrariTRT (Ferrari Turbo Mode - FP32)")
    TRACKER_CLASS = OSTrackFerrariTRT
    TRACKER_NAME = "OSTrack Ferrari (TensorRT Turbo - FP32)"
except ImportError as e:
    print(f"âŒ Error importing TensorRT or Tracker: {e}")
    print("âŒ TensorRT not found! Falling back to standard OSTrackFerrari, but this is app_TensorRT.py!")
    from trackers.ostrack_tracker import OSTrackFerrari
    TRACKER_CLASS = OSTrackFerrari
    TRACKER_NAME = "OSTrack Ferrari (Standard - TRT Missing)"


# Configure Gradio temp directory to avoid C drive usage
os.environ['GRADIO_TEMP_DIR'] = os.path.join(current_dir, 'gradio_temp')
Path(os.environ['GRADIO_TEMP_DIR']).mkdir(exist_ok=True)

# Global tracker instance
tracker = None
OUTPUT_DIR = Path("output")
OUTPUT_DIR.mkdir(exist_ok=True)

# ROI Selection State
roi_state = {
    'frame': None,
    'points': [],
    'bbox': None
}


def process_video(video_path, x1, y1, x2, y2):
    """
    Main tracking pipeline
    """
    global tracker
    
    # Validate inputs
    if video_path is None:
        return None, "âŒ Error: Please upload a video first!"
    
    if x1 is None or y1 is None or x2 is None or y2 is None:
        return None, "âŒ Error: Please select a target region!"
    
    try:
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
    except:
        return None, "âŒ Error: Invalid bbox coordinates!"
    
    # Ensure valid bbox
    if x2 <= x1 or y2 <= y1:
        return None, "âŒ Error: Invalid bbox! x2 must be > x1, y2 > y1"
    
    init_bbox = (x1, y1, x2-x1, y2-y1)
    
    # Open video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return None, "âŒ Error: Cannot open video file!"
    
    # Get video properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Output video writer
    timestamp = int(time.time())
    video_stem = Path(video_path).stem
    output_path = OUTPUT_DIR / f"{video_stem}_trt_fp32_{timestamp}.mp4"
    
    # Explicitly cleanup old tracker to free CUDA context
    if tracker is not None:
        if hasattr(tracker, 'cleanup'):
            tracker.cleanup()
        del tracker
        tracker = None
        
    # Read first frame
    ret, first_frame = cap.read()
    if not ret:
        return None, "âŒ Error: Cannot read first frame!"
    
    tracker = TRACKER_CLASS()
    tracker.init(first_frame, init_bbox)
    
    # Try codecs in order of preference for browser compatibility
    codecs_to_try = ['avc1', 'h264', 'mp4v']
    out = None
    
    for codec in codecs_to_try:
        fourcc = cv2.VideoWriter_fourcc(*codec)
        temp_out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
        if temp_out.isOpened():
            print(f"âœ… VideoWriter initialized with codec: {codec}")
            out = temp_out
            break
            
    if out is None:
        return None, "âŒ Error: Failed to initialize VideoWriter with any codec!"
        
    # Draw and write the first frame (Initial Target)
    vis_frame = first_frame.copy()
    cv2.rectangle(vis_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
    cv2.putText(vis_frame, "Initial Target", (x1, y1-10), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    out.write(vis_frame)
    
    # è·Ÿè¸ªç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_frames': total_frames,
        'tracked': 0,
        'lost': 0,
        'redetected': 0,
        'avg_fps': 0,
        'avg_confidence': 0
    }
    
    frame_idx = 1
    confidences = []
    start_time = time.time()
    
    print(f"ğŸ¯ [TRT-FP32] å¼€å§‹è·Ÿè¸ªï¼Œåˆå§‹è¾¹ç•Œæ¡†: {init_bbox}")
    
    # å¤„ç†å‰©ä½™å¸§
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # è·Ÿè¸ªå¯¹è±¡
        bbox, confidence, status = tracker.update(frame)
        confidences.append(confidence)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if status == "REDETECTED":
            stats['redetected'] += 1
        elif status == "LOST":
            stats['lost'] += 1
        else:
            stats['tracked'] += 1
        
        # å¯è§†åŒ–ï¼ˆä»…å½“ç½®ä¿¡åº¦ > 0.10 æ—¶æ˜¾ç¤ºè¾¹ç•Œæ¡†ï¼‰
        vis_frame = frame.copy()
        if bbox is not None and confidence > 0.10:
            x, y, w, h = [int(v) for v in bbox]
            
            # åŸºäºç½®ä¿¡åº¦è®¾å®šé¢œè‰²
            if confidence > 0.5:
                color = (0, 255, 0)  # ç»¿è‰²
            elif confidence > 0.35:
                color = (0, 165, 255)  # æ©™è‰²
            else:
                color = (0, 0, 255)  # çº¢è‰²
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(vis_frame, (x, y), (x+w, y+h), color, 2)
            
            # çŠ¶æ€æ–‡æœ¬
            text = f"{status} | Conf: {confidence:.2f}"
            cv2.putText(vis_frame, text, (x, y-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # å¸§è®¡æ•°å™¨ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼‰
        cv2.putText(vis_frame, f"Frame: {frame_idx}/{total_frames} (TRT-FP32)", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        out.write(vis_frame)
        frame_idx += 1
        
        # è¿›åº¦æŒ‡ç¤ºå™¨
        if frame_idx % 30 == 0:
            progress = (frame_idx / total_frames) * 100
            print(f"å¤„ç†è¿›åº¦: {progress:.1f}% | çŠ¶æ€: {status} | ç½®ä¿¡åº¦: {confidence:.2f}")
    
    # æ¸…ç†èµ„æº
    cap.release()
    out.release()
    
    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    elapsed_time = time.time() - start_time
    stats['avg_fps'] = total_frames / elapsed_time if elapsed_time > 0 else 0
    stats['avg_confidence'] = np.mean(confidences) if confidences else 0
    
    stats_text = f"""
    âœ… è·Ÿè¸ªå®Œæˆ (TensorRT FP32)ï¼
    ğŸ“Š ç»Ÿè®¡æ•°æ®:
    - æ€»å¸§æ•°: {stats['total_frames']}
    - æˆåŠŸè·Ÿè¸ªå¸§æ•°: {stats['tracked']}
    - é‡æ£€æµ‹æ¬¡æ•°: {stats['redetected']}
    - ä¸¢å¤±å¸§æ•°: {stats['lost']}
    - å¹³å‡ FPS: {stats['avg_fps']:.1f}
    - å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.3f}    
    ğŸ¥ è¾“å‡ºæ–‡ä»¶: {output_path.name}
    """
    
    # ä¿å­˜ç»Ÿè®¡æ•°æ®åˆ°æ–‡ä»¶
    video_stem = Path(video_path).stem
    stats_file_path = OUTPUT_DIR / f"{video_stem}_trt_fp32.txt"
    try:
        with open(stats_file_path, "w", encoding="utf-8") as f:
            f.write(stats_text)
        print(f"ğŸ“Š ç»Ÿè®¡æ•°æ®å·²ä¿å­˜è‡³: {stats_file_path}")
        stats_text += f"\nğŸ“„ ç»Ÿè®¡æ–‡ä»¶: {stats_file_path.name}"
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»Ÿè®¡æ–‡ä»¶å¤±è´¥: {e}")
    
    return str(output_path), stats_text


def load_first_frame(video_path):
    global roi_state
    if video_path is None:
        roi_state['frame'] = None
        roi_state['points'] = []
        roi_state['bbox'] = None
        return None, None, None, None, None, None, None
    cap = cv2.VideoCapture(video_path)
    ret, frame = cap.read()
    cap.release()
    
    if ret:
        # å…¨å±€å­˜å‚¨å¸§
        roi_state['frame'] = frame
        roi_state['points'] = []
        roi_state['bbox'] = None
        
        # å°† BGR è½¬æ¢ä¸º RGB ä»¥ä¾› Gradio ä½¿ç”¨
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        height, width = frame.shape[:2]
        
        # å°†è¾¹ç•Œæ¡†åˆå§‹åŒ–ä¸ºå¸§ä¸­å¿ƒçš„å››åˆ†ä¹‹ä¸€
        default_x1 = width // 4
        default_y1 = height // 4
        default_x2 = 3 * width // 4
        default_y2 = 3 * height // 4
        # åœ¨å¸§ä¸Šç»˜åˆ¶åˆå§‹è¾¹ç•Œæ¡†
        preview = rgb_frame.copy()
        cv2.rectangle(preview, (default_x1, default_y1), (default_x2, default_y2), (0, 255, 0), 2)
        # æ³¨æ„ï¼šæ­¤å¤„ä¸ºåœ¨Gradioä¸­æ˜¾ç¤ºçš„å›¾ç‰‡ï¼Œè‹¥OpenCVä¸æ”¯æŒä¸­æ–‡ï¼Œä¾ç„¶ä½¿ç”¨è‹±æ–‡æç¤º
        cv2.putText(preview, "Adjust bbox below or click two points", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        return preview, default_x1, default_y1, default_x2, default_y2, width, height
    
    return None, None, None, None, None, None, None


def update_bbox_preview(video_path, x1, y1, x2, y2):
    global roi_state
    if roi_state['frame'] is None:
        return None
    # è½¬æ¢ä¸º RGB
    preview = cv2.cvtColor(roi_state['frame'], cv2.COLOR_BGR2RGB).copy()
    # å¦‚æœè¾¹ç•Œæ¡†æœ‰æ•ˆåˆ™ç»˜åˆ¶
    try:
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
        if x2 > x1 and y2 > y1:
            cv2.rectangle(preview, (x1, y1), (x2, y2), (0, 255, 0), 3)
            # ç»˜åˆ¶åæ ‡æ–‡æœ¬
            cv2.putText(preview, f"ROI: ({x1},{y1}) to ({x2},{y2})", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            w, h = x2 - x1, y2 - y1
            cv2.putText(preview, f"Size: {w}x{h}", (10, 60),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    except:
        pass
    return preview


def handle_image_click(video_path, x1, y1, x2, y2, evt: gr.SelectData):
    """
    å¤„ç†å›¾åƒç‚¹å‡»äº‹ä»¶ä»¥è®¾ç½®è¾¹ç•Œæ¡†è§’ç‚¹
    """
    global roi_state
    
    if roi_state['frame'] is None:
        return x1, y1, x2, y2
    
    click_x, click_y = evt.index[0], evt.index[1]
    roi_state['points'].append((click_x, click_y))
    
    print(f"ğŸ–±ï¸ ç‚¹å‡»ç¬¬ {len(roi_state['points'])} æ¬¡: ({click_x}, {click_y})")
    
    # ç‚¹å‡»ä¸¤æ¬¡åï¼Œè®¾ç½®è¾¹ç•Œæ¡†
    if len(roi_state['points']) >= 2:
        p1, p2 = roi_state['points'][-2], roi_state['points'][-1]
        new_x1 = min(p1[0], p2[0])
        new_y1 = min(p1[1], p2[1])
        new_x2 = max(p1[0], p2[0])
        new_y2 = max(p1[1], p2[1])
        
        print(f"âœ“ è¾¹ç•Œæ¡†å·²è®¾å®š: ({new_x1},{new_y1}) è‡³ ({new_x2},{new_y2})")
        
        return new_x1, new_y1, new_x2, new_y2
    
    return x1, y1, x2, y2


# Gradio ç•Œé¢
with gr.Blocks(title="ğŸ¯ é›¶æ ·æœ¬ç›®æ ‡è·Ÿè¸ªå™¨ (TensorRT FP32)") as demo:
    gr.Markdown(f"""
    # ğŸï¸ {TRACKER_NAME}
    
    **åŠŸèƒ½ç‰¹ç‚¹:**
    - âœ¨ è·Ÿè¸ªä»»æ„ç›®æ ‡ (é›¶æ ·æœ¬/Zero-shot)
    - ğŸš€ **TensorRT åŠ é€Ÿ (FP32)**
    - ğŸ”„ é®æŒ¡æ¢å¤èƒ½åŠ›
    - ğŸ¨ æé€Ÿæ¨ç†
    
    **ä½¿ç”¨è¯´æ˜:**
    1. ä¸Šä¼ ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ã€‚
    2. **æ–¹æ³• 1**: åœ¨è§†é¢‘å¸§ä¸Šç‚¹å‡» **ä¸¤ç‚¹** æ¥å®šä¹‰è¾¹ç•Œæ¡†ï¼ˆå·¦ä¸Šè§’å’Œå³ä¸‹è§’ï¼‰ã€‚
    3. **æ–¹æ³• 2**: è°ƒæ•´å›¾åƒä¸‹æ–¹çš„åæ ‡æ»‘å—ã€‚
    4. ç‚¹å‡» "å¼€å§‹è·Ÿè¸ª" (Start Tracking)ã€‚
    5. ä¸‹è½½è·Ÿè¸ªå®Œæˆçš„è§†é¢‘ã€‚
    """)
    
    with gr.Row():
        with gr.Column():
            video_input = gr.Video(label="ğŸ“¤ ä¸Šä¼ è§†é¢‘")
            
            frame_display = gr.Image(
                label="ğŸ¯ ç‚¹å‡»ä¸¤ç‚¹é€‰æ‹© ROIï¼ˆæˆ–ä½¿ç”¨ä¸‹æ–¹æ»‘å—è°ƒæ•´ï¼‰",
                interactive=False,
                type="numpy"
            )
            
            with gr.Row():
                x1_slider = gr.Number(label="X1 (å·¦)", value=0, precision=0)
                y1_slider = gr.Number(label="Y1 (é¡¶)", value=0, precision=0)
            
            with gr.Row():
                x2_slider = gr.Number(label="X2 (å³)", value=100, precision=0)
                y2_slider = gr.Number(label="Y2 (åº•)", value=100, precision=0)
            
            # éšè—çš„å°ºå¯¸çŠ¶æ€
            frame_width = gr.State(value=None)
            frame_height = gr.State(value=None)
            
            track_btn = gr.Button("ğŸš€ å¼€å§‹è·Ÿè¸ª (TensorRT FP32)", variant="primary", size="lg")
        
        with gr.Column():
            output_video = gr.Video(label="ğŸ“¥ è·Ÿè¸ªç»“æœè§†é¢‘")
            stats_output = gr.Textbox(label="ğŸ“Š è·Ÿè¸ªç»Ÿè®¡ä¿¡æ¯", lines=12)
    
    # äº‹ä»¶å¤„ç†
    video_input.change(
        fn=load_first_frame,
        inputs=[video_input],
        outputs=[frame_display, x1_slider, y1_slider, x2_slider, y2_slider, frame_width, frame_height]
    )
    
    # å½“æ»‘å—å˜åŒ–æ—¶æ›´æ–°é¢„è§ˆ
    for slider in [x1_slider, y1_slider, x2_slider, y2_slider]:
        slider.change(
            fn=update_bbox_preview,
            inputs=[video_input, x1_slider, y1_slider, x2_slider, y2_slider],
            outputs=[frame_display]
        )
    
    # å¤„ç†å›¾åƒç‚¹å‡»
    frame_display.select(
        fn=handle_image_click,
        inputs=[video_input, x1_slider, y1_slider, x2_slider, y2_slider],
        outputs=[x1_slider, y1_slider, x2_slider, y2_slider]
    )
    
    # å¼€å§‹è·Ÿè¸ª
    track_btn.click(
        fn=process_video,
        inputs=[video_input, x1_slider, y1_slider, x2_slider, y2_slider],
        outputs=[output_video, stats_output]
    )
    
    gr.Markdown("""
    ---
    ### ğŸ”§ æŠ€æœ¯æ ˆ:
    - **ä¸»è·Ÿè¸ªå™¨**: OSTrack (TensorRT Engine)
    - **ç²¾åº¦**: FP32 (High Precision)
    - **åŠ é€Ÿ**: NVIDIA TensorRT
    """)


if __name__ == "__main__":
    demo.launch(share=False, server_name="0.0.0.0", server_port=8579) # Use different port
