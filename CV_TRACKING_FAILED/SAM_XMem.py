import gradio as gr
import cv2
import numpy as np
import torch
import sys
import os
from segment_anything import sam_model_registry, SamPredictor

# ---------------------------------------------------------
# 1. ç³»ç»Ÿé…ç½®ä¸æ¨¡å‹åŠ è½½ (System Config & Model Loading)
# ---------------------------------------------------------

# è‡ªåŠ¨é€‰æ‹©è®¡ç®—è®¾å¤‡
# [B, C, H, W] ä¸Šä¸‹æ–‡ä¸­çš„ Device é€‰æ‹©
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {DEVICE}")

# æ¨¡å‹ç±»å‹é€‰æ‹© (vit_b æ˜¯é€Ÿåº¦å’Œç²¾åº¦çš„å¹³è¡¡ç‚¹)
MODEL_TYPE = "vit_b"
# è¯·ç¡®ä¿è¯¥æ–‡ä»¶å­˜åœ¨ï¼Œæˆ–è€…ä¿®æ”¹è·¯å¾„
CHECKPOINT_PATH = "sam_vit_b_01ec64.pth"


def load_sam_model():

    if not os.path.exists(CHECKPOINT_PATH):
        print(f"Error: Checkpoint {CHECKPOINT_PATH} not found.")
        print("Please download it from: https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth")
        # ä¸ºäº†æ¼”ç¤ºä¸æŠ¥é”™ï¼Œè¿™é‡Œè¿”å› Noneï¼Œå®é™…è¿è¡Œéœ€è¦ä¸‹è½½
        return None

    try:
        sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)
        sam.to(device=DEVICE)
        predictor = SamPredictor(sam)
        return predictor
    except Exception as e:
        print(f"Error loading SAM: {e}")
        return None



sam_predictor = load_sam_model()


# ---------------------------------------------------------
# 2. ç®—æ³•æ ¸å¿ƒé€»è¾‘ (Core Algorithms)
# ---------------------------------------------------------

def get_box_from_mask(mask):

    pos = np.where(mask > 0)
    if len(pos[0]) == 0:
        return None
    return np.array([np.min(pos[1]), np.min(pos[0]), np.max(pos[1]), np.max(pos[0])])


def run_tracking_sam(video_path, user_bbox_data, progress=gr.Progress()):
    """
    æ ¸å¿ƒè·Ÿè¸ªå¾ªç¯ï¼šSAM + Optical Flow

    ç†è®ºåˆ†æ (Complexity Analysis):
    - SAM Encoder: O(H*W) - Transformer åŸºäº Patchï¼Œè®¡ç®—é‡å·¨å¤§ã€‚
    - Optical Flow (LK): O(N * K^2) - N æ˜¯ç‰¹å¾ç‚¹æ•°ï¼ŒK æ˜¯çª—å£å¤§å°ã€‚
    - æ•´ä½“å¤æ‚åº¦ï¼šæ¯å¸§éƒ½è·‘ SAM Encoder æ˜¯ç“¶é¢ˆã€‚
    """
    if sam_predictor is None:
        return None, "Error: SAM Model not loaded. Check checkpoint path."

    if video_path is None or user_bbox_data is None:
        return None, "Error: No video or box detected."

    # 1. è§£æç”¨æˆ·è¾“å…¥ (Parse User Input)
    # Gradio çš„ ImageEditor è¿”å›çš„æ•°æ®ç»“æ„
    mask_input = user_bbox_data["layers"][0]  # [H, W, 4] or [H, W]

    # å¦‚æœæ˜¯ RGBAï¼Œè½¬ä¸ºå•é€šé“ Mask
    if len(mask_input.shape) == 3:
        mask_input = mask_input[:, :, 0]

    init_box = get_box_from_mask(mask_input)
    if init_box is None:
        return video_path, "No object selected."

    # 2. è§†é¢‘æµåˆå§‹åŒ–
    cap = cv2.VideoCapture(video_path)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # é™é‡‡æ ·ä»¥åŠ å¿«å¤„ç†é€Ÿåº¦ (SAM å¯¹é«˜åˆ†è¾¨ç‡å›¾åƒæ¨ç†è¾ƒæ…¢)
    # [H, W, C]
    resize_factor = 0.5
    proc_w, proc_h = int(width * resize_factor), int(height * resize_factor)

    # è¾“å‡ºè§†é¢‘é…ç½®
    output_path = "tracked_output.mp4"
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    # 3. çŠ¶æ€åˆå§‹åŒ–
    # current_box: [x1, y1, x2, y2] (åœ¨åŸå§‹åˆ†è¾¨ç‡ä¸‹)
    current_box = init_box
    prev_gray = None
    prev_pts = None

    # é¢œè‰²å®šä¹‰
    mask_color = np.array([30, 144, 255], dtype=np.uint8)  # DodgerBlue

    for i in progress.tqdm(range(total_frames), desc="Tracking Objects"):
        ret, frame = cap.read()
        if not ret:
            break

        # åŸå§‹å¸§ç”¨äºæ˜¾ç¤ºï¼Œç¼©å°å¸§ç”¨äºå…‰æµè®¡ç®—
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        frame_gray_small = cv2.resize(frame_gray, (proc_w, proc_h))

        # ---------------------------------------------------------
        # é˜¶æ®µ A: ä½ç½®é¢„æµ‹ (Position Prediction via Optical Flow)
        # ç±»ä¼¼äº Kalman Filter çš„ Predict æ­¥éª¤ï¼Œä½†åˆ©ç”¨åƒç´ çº§ç‰¹å¾
        # ---------------------------------------------------------
        if prev_gray is not None and prev_pts is not None:
            # Lucas-Kanade Optical Flow
            # p1: [N, 1, 2] (New points)
            # st: Status (1 if found)
            # err: Error
            p1, st, err = cv2.calcOpticalFlowPyrLK(
                prev_gray, frame_gray_small, prev_pts, None,
                winSize=(15, 15), maxLevel=2,
                criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
            )

            # é€‰æ‹©å¥½çš„è·Ÿè¸ªç‚¹
            good_new = p1[st == 1]
            good_old = prev_pts[st == 1]

            if len(good_new) > 5:  # å¦‚æœæœ‰è¶³å¤Ÿçš„ç‚¹è¢«è·Ÿè¸ª
                # è®¡ç®—å¹³å‡ä½ç§»å‘é‡ (dx, dy)
                movement = np.mean(good_new - good_old, axis=0)
                # è¿˜åŸå›åŸå§‹åˆ†è¾¨ç‡
                movement /= resize_factor

                # æ›´æ–° Box ä½ç½® (ç®€å•çš„å¹³ç§»)
                # [x1, y1, x2, y2]
                current_box[0] += movement[0]
                current_box[1] += movement[1]
                current_box[2] += movement[0]
                current_box[3] += movement[1]
            else:
                # è·Ÿè¸ªä¸¢å¤±å¤„ç†ï¼šè¿™é‡Œç®€å•ä¿æŒåŸä½æˆ–æ‰©å¤§æœç´¢èŒƒå›´
                pass

        # ---------------------------------------------------------
        # é˜¶æ®µ B: SAM ç»†åŒ– (Refinement via SAM)
        # ç±»ä¼¼äº Kalman Filter çš„ Update æ­¥éª¤ï¼Œåˆ©ç”¨è§‚æµ‹å€¼(Prompt)ä¿®æ­£
        # ---------------------------------------------------------

        # å¿…é¡»è°ƒç”¨ set_imageï¼Œè¿™æ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†
        # SAM éœ€è¦ [H, W, 3] RGB input
        sam_predictor.set_image(frame_rgb)

        # ä½¿ç”¨é¢„æµ‹çš„ Box ä½œä¸º Prompt
        input_box = current_box[None, :]  # å¢åŠ  Batch ç»´åº¦ -> [1, 4]

        masks, scores, logits = sam_predictor.predict(
            point_coords=None,
            point_labels=None,
            box=input_box,
            multimask_output=False,  # åªè¿”å›è¿™ä¸€ä¸ªç‰©ä½“
        )

        # masks: [1, H, W] boolean
        best_mask = masks[0]

        # ---------------------------------------------------------
        # é˜¶æ®µ C: çŠ¶æ€æ›´æ–° (State Update)
        # ä¸ºä¸‹ä¸€å¸§å‡†å¤‡å…‰æµç‰¹å¾ç‚¹
        # ---------------------------------------------------------

        # æ ¹æ®æ–°çš„ Mask æ›´æ–° Boxï¼Œé˜²æ­¢æ¼‚ç§»
        refined_box = get_box_from_mask(best_mask)

        if refined_box is not None:
            # å¯ä»¥åœ¨è¿™é‡ŒåšåŠ¨é‡å¹³æ»‘ (Momentum Smoothing)
            # current_box = alpha * current_box + (1-alpha) * refined_box
            current_box = refined_box

            # æå– Mask å†…éƒ¨çš„ç‰¹å¾ç‚¹ç”¨äºä¸‹ä¸€å¸§å…‰æµ
            # 1. åˆ›å»º Mask çš„ç¼©å°ç‰ˆ
            mask_small = cv2.resize(best_mask.astype(np.uint8), (proc_w, proc_h))

            # 2. ä»…åœ¨ Mask åŒºåŸŸå†…å¯»æ‰¾è§’ç‚¹ (Shi-Tomasi Corner Detector)
            prev_pts = cv2.goodFeaturesToTrack(
                frame_gray_small, mask=mask_small,
                maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7
            )

        prev_gray = frame_gray_small

        # ---------------------------------------------------------
        # é˜¶æ®µ D: å¯è§†åŒ– (Visualization)
        # ---------------------------------------------------------

        # å åŠ  Mask
        # Mask [H, W] -> [H, W, 1]
        if refined_box is not None:
            viz_mask = best_mask[:, :, np.newaxis]
            # ç®€å•çš„åŠé€æ˜å åŠ 
            overlay = frame.copy()
            overlay[best_mask] = (frame[best_mask] * 0.5 + mask_color * 0.5).astype(np.uint8)
            frame = overlay

            # ç”»æ¡†
            p1, p2 = (int(current_box[0]), int(current_box[1])), (int(current_box[2]), int(current_box[3]))
            cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)
            cv2.putText(frame, "SAM Tracker", (p1[0], p1[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

        out.write(frame)

    cap.release()
    out.release()

    return output_path, "Tracking Completed."


def get_first_frame(video_path):
    if video_path is None: return None
    cap = cv2.VideoCapture(video_path)
    ret, frame = cap.read()
    cap.release()
    if ret:
        return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    return None


# ---------------------------------------------------------
# 3. Gradio ç•Œé¢æ„å»º
# ---------------------------------------------------------

with gr.Blocks(title="Zero-Shot SAM Tracker") as app:
    gr.Markdown("# ğŸ§¬ Zero-Shot Object Tracking (SAM + Optical Flow)")
    gr.Markdown("""
    **æ ¸å¿ƒæœºåˆ¶**: ä½¿ç”¨ Segment Anything Model (SAM) è·å–é«˜è´¨é‡åˆ†å‰²ï¼Œåˆ©ç”¨å…‰æµæ³• (Optical Flow) æ¨¡æ‹Ÿæ—¶åºè®°å¿†ä¼ é€’ Promptã€‚
    **æ³¨æ„**: éœ€è¦ä¸‹è½½ `sam_vit_b_01ec64.pth` æƒé‡æ–‡ä»¶ã€‚
    """)

    with gr.Row():
        with gr.Column():
            video_input = gr.Video(label="1. ä¸Šä¼ è§†é¢‘ (Upload Video)")
            extract_btn = gr.Button("2. è·å–ç¬¬ä¸€å¸§ (Get Frame)")
            # ImageEditor ç”¨äºç”»æ¡†
            image_input = gr.ImageEditor(
                label="3. æ¶‚æŠ¹ç›®æ ‡ (Paint over Target)",
                type="numpy",
                brush=gr.Brush(colors=["#FFFFFF"], default_size=20),
                interactive=True
            )
            track_btn = gr.Button("4. å¼€å§‹è·Ÿè¸ª (Start Tracking)", variant="primary")

        with gr.Column():
            video_output = gr.Video(label="è·Ÿè¸ªç»“æœ (Result)")
            status_text = gr.Textbox(label="çŠ¶æ€ (Status)", interactive=False)

    extract_btn.click(fn=get_first_frame, inputs=video_input, outputs=image_input)
    track_btn.click(
        fn=run_tracking_sam,
        inputs=[video_input, image_input],
        outputs=[video_output, status_text]
    )

# ---------------------------------------------------------
# 4. CLI è¿è¡Œå£°æ˜ (CLI Command)
# ---------------------------------------------------------
if __name__ == "__main__":
    print("Starting SAM Tracker App...")
    print("Command: python tracker_sam_app.py")
    app.launch(share=False)